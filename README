pip install  --trusted-host pypi.org --trusted-host files.pythonhosted.org scikit-learn shap fairlearn evidently joblib


1. Train and Save the Model
Run the train_model.py script to:

Train the model
Then Save model and data splits into ./data/

 Summary: Same File Format, Different Contents
I saved in below formats for Local dev or testing	Quick save/load, fast prototyping
model.joblib	==> A trained ML model object (e.g., RandomForestClassifier)	For making predictions
data_splits.joblib	===> A tuple of NumPy arrays: (X_train, X_test, y_train, y_test)	For model evaluation and testing

====================
But in real production situations, you would typically save the model in a more robust format like ONNX or PMML for better compatibility and deployment.
in Real Production Environments
Approach	Used For	Tools or Formats
Model Serving API	===>Real-time predictions	FastAPI, Flask, BentoML
Model Registry	===>Versioning, approvals, rollbacks ===>	MLflow, SageMaker, Vertex AI
Serialized Formats===>	Cross-platform model serving	===> ONNX, PMML, TorchScript
Docker Containers===>	Full inference environment packaging	===> Docker + Kubernetes
Streaming Pipelines ===>	Live scoring at scale	===> Kafka, Spark Streaming
====================
Best Practice Workflow
Train model locally

Save as .joblib or .pkl

Register in model registry

Track model, metrics, metadata

Package model for deployment

Export to ONNX or wrap in API container

Deploy via CI/CD

Push to staging or production

Monitor in production

Track input drift, latency, prediction health


=======================================# Run the training script

2. run main.py  # for  Run pre-prod deployment testing
===========

========================
3. post prod deployment testing
python simulate_postprod_test.py  ( for iris dataset generated model)
===================================
python simulate_postprod_drift_test.py # for mismatched dataset generated model -- details below
Details: Load Mismatched Dataset as Live Data
Use a completely different dataset with similar shape to simulate “bad live data”.

Example using load_wine() (same shape columns):

==============================


DRIFT testing results:
1. current_accuracy: 0.45
Your model correctly predicted only 45% of the recent ("live") samples.

This is a significant drop from a typically expected accuracy (e.g., >80%).

 2. prediction_drift_rate: 0.55
55% of the predictions made on current data are different from previous predictions (or from a stable model).
This is a high drift rate, suggesting that the model is making substantially different decisions now.
Could be caused by: changes in feature values, model instability, new data patterns.


Next Steps You Should Consider
Retrain or fine-tune the model using recent data
Investigate segment-specific features (especially for East)
Trigger alerts or rollback mechanisms if thresholds are breached
Audit data pipelines for changes in input format or feature values
Use SHAP or LIME to explain misclassifications in degraded segments

===========

